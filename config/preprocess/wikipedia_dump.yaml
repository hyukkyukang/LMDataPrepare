defaults:
  - _base
  - _self_

dataset:
  name: wikipedia_dump
  source: wikimedia
  version: latest
  language: en
  dump_date: latest
  require_high_fidelity_parser: true
  namespace: 0
  include_redirects: false

download:
  source_url: https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2
  target_dir: data/raw/wikipedia
  filename: enwiki-latest-pages-articles-multistream.xml.bz2
  expected_sha256: null
  expected_size_bytes: null
  retries: 3
  retry_backoff_seconds: 5
  timeout_seconds: 30
  overwrite: false

processing:
  num_proc: ${oc.env:PREPROCESS_NUM_PROC,16}
  chunk_size: 2000
  min_body_chars: 100
  drop_empty_title: true
  normalize_whitespace: true

filtering:
  body_field: body
  title_field: title
  canonical_fields:
    - title
    - body
  require_non_empty_title: ${processing.drop_empty_title}
  min_body_chars: ${processing.min_body_chars}
  max_body_chars: null
  enable_pii_redaction: true
  pii_block_on_match: false
  boilerplate_patterns:
    - "\\[\\s*edit\\s*\\]"

dedup:
  exact:
    enabled: true
  near:
    enabled: true
    hamming_threshold: 3
    max_candidates_per_doc: 128

quality:
  enabled: false

output:
  output_dir: data/processed/wikipedia_dump
  file_prefix: wikipedia-en
  shard_target_rows: 200000
  parquet_compression: zstd
  parquet_extension: parquet
  manifest_name: manifest.json
  summary_name: run_summary.json
  dataset_readme_name: README.md

hub:
  push: false
  repo_id: null
  private: true
  revision: main
  token_env_var: HF_TOKEN
  commit_message: "Add preprocessed wikipedia dump shards"
  path_in_repo: data
